{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb277093",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea1e42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7944935",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91b22e",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "317c2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to images\n",
    "image_path_arnau = \"data/Arnau\"\n",
    "image_path_ashley = \"data/Ashley\"\n",
    "\n",
    "# Prepare list with image paths\n",
    "file_list_arnau = os.listdir(image_path_arnau)\n",
    "file_list_arnau = [ f for f in file_list_arnau if f.endswith('.jpeg') or f.endswith('.jpg')]\n",
    "file_list_ashley = os.listdir(image_path_ashley)\n",
    "file_list_ashley = [ f for f in file_list_ashley if f.endswith('.jpeg') or f.endswith('.jpg')]\n",
    "\n",
    "# Join lists and create labels\n",
    "file_list = file_list_arnau + file_list_ashley\n",
    "labels = np.array([0] * len(file_list_arnau) + [1] * len(file_list_ashley)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f75f3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(file_list_ashley)):\n",
    "    path = image_path_ashley + '/' + file_list_ashley[4]\n",
    "    img = cv2.imread(path)\n",
    "    test = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6ab4a",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40727c",
   "metadata": {},
   "source": [
    "### Functions for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db1b710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find center of boundary box and return it.\n",
    "def findCenterBbox(bbox):\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_max = x_min + w\n",
    "    y_max = y_min + h\n",
    "\n",
    "    center = [int((x_max + x_min)//2), int((y_max + y_min)//2)] \n",
    "\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2b1da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find face using the OpenCV library and return boundary box.\n",
    "def findFaces(img):\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faceDetection = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    results = faceDetection.detectMultiScale(imgGray, 1.1, 10, minSize=[int(0.25*imgGray.shape[0]),int(0.25*imgGray.shape[1])])\n",
    "\n",
    "    bboxs = []\n",
    "    \n",
    "    if len(results):\n",
    "        counter = 1\n",
    "        for bbox in results:\n",
    "            center_bbox = findCenterBbox(bbox)\n",
    "            bboxs.append([counter, bbox, center_bbox])\n",
    "            counter = counter + 1\n",
    "\n",
    "    return bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "988f742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dimensions of a square box based on the center and the dimensions.\n",
    "def makeSquareCrop(x_min, y_min, w, h):\n",
    "    centerX = x_min + round(w/2)\n",
    "    centerY = y_min + round(h/2)\n",
    "    dim = max([w,h])\n",
    "\n",
    "    x_min = centerX - round(dim/2)\n",
    "    y_min = centerY - round(dim/2)\n",
    "\n",
    "    w = dim\n",
    "    h = dim\n",
    "\n",
    "    return x_min, y_min, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "112ac4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the boundaries are valid.\n",
    "def boundariesValidation(x, y):\n",
    "    x = max([x, 0])\n",
    "    y = max([y, 0])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61c9c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop image and return it.\n",
    "def crop(img, bbox):\n",
    "    x_min, y_min, w, h = bbox\n",
    "\n",
    "    x_min, y_min, w, h = makeSquareCrop(x_min, y_min, w, h)        \n",
    "\n",
    "    x_min, y_min = boundariesValidation(x_min, y_min)\n",
    "\n",
    "    return img[y_min:y_min+h, x_min:x_min+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bc12f",
   "metadata": {},
   "source": [
    "### Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbda8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▏                                                                                                                    | 21/1159 [00:01<01:07, 16.92it/s]"
     ]
    }
   ],
   "source": [
    "input_size = [32,32] # Input size of image\n",
    "\n",
    "# Initialize img_feature to store the images\n",
    "img_features = np.empty([1,input_size[0], input_size[1],3])\n",
    "idx_keep = []\n",
    "\n",
    "# For loop to iterate through all files\n",
    "for i in tqdm(range(1,len(file_list))):\n",
    "\n",
    "    f = file_list[i] # Current file\n",
    "    \n",
    "    # Path based on label\n",
    "    if labels[i]:\n",
    "        path = image_path_ashley + '/' + f\n",
    "    else:\n",
    "        path = image_path_arnau + '/' + f\n",
    "\n",
    "    # Skip iteration if image cannot be loaded\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    bboxs = findFaces(img) # Find face\n",
    "\n",
    "    # If no faces have been found, skip face\n",
    "    if(len(bboxs) == 0):\n",
    "        continue     \n",
    "    \n",
    "    # Crop faces in square dimensions\n",
    "    img_face = crop(img, bboxs[0][1])\n",
    "    \n",
    "    # Resize image into input_size and convert BGR to RGB\n",
    "    img_face_ds = cv2.resize(img_face, (input_size[0],input_size[1]))\n",
    "    img_face_ds = cv2.cvtColor(img_face_ds, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Reshape image and append it to img_features\n",
    "    features = np.reshape(img_face_ds,[1,input_size[0],input_size[1],3])\n",
    "    img_features = np.append(img_features,features,axis=0)\n",
    "\n",
    "    idx_keep.append(i) # Indices of images with detected faces\n",
    "\n",
    "# Delete positions where no faces have been detected\n",
    "img_features = np.delete(img_features,obj=0,axis=0)\n",
    "labels = labels[idx_keep]\n",
    "\n",
    "N_samples = img_features.shape[0] # Compute number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee990cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = img_features[443,:,:,:]\n",
    "#plt.imshow((out * 255).astype(np.uint8))\n",
    "#plt.show()\n",
    "#img_features[0,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7657d",
   "metadata": {},
   "source": [
    "# Deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe097b",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN) architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "    tf.keras.layers.Conv2D(input_size[0], (3, 3), activation='relu', input_shape=(input_size[0], input_size[1], 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de9d99",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b158eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 1/10 [==>...........................] - ETA: 3s - loss: 0.0000e+00 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 11:40:55.111191: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 43ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      " 6/10 [=================>............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 11:40:55.549976: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "5/5 - 0s - loss: 0.0000e+00 - accuracy: 1.0000 - 29ms/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define indices of training (70%) and testing (30%)\n",
    "train_indices = random.sample(range(0, N_samples), round(N_samples*0.7)) #int(np.floor(img_features.shape[0]*0.7))\n",
    "test_indices = list(set(list(range(0,N_samples))).difference(set(train_indices)))\n",
    "\n",
    "# Split images into training and testing\n",
    "train = img_features[train_indices,:,:,:]\n",
    "train_labels = labels[train_indices]\n",
    "\n",
    "test = img_features[test_indices,:,:,:]\n",
    "test_labels = labels[test_indices]\n",
    "\n",
    "# Train CNN model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train, train_labels, epochs=2, validation_data=(test, test_labels))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('data/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304210be",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b173404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 11:12:48.047142: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('data/my_model') # Import model\n",
    "cap = cv2.VideoCapture(0) # Load webcam\n",
    "class_names = ['Arnau', 'Ashley'] # Classes names\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Real-time face recognition\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=0.5, fy=0.5,\n",
    "                       interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    bboxs = findFaces(frame)\n",
    "\n",
    "    c = cv2.waitKey(1)\n",
    "\n",
    "    if c == 27:  # Escape\n",
    "        break\n",
    "\n",
    "    for i in range(0, len(bboxs)):\n",
    "        bbox = bboxs[i][1]\n",
    "        img_face = crop(frame, bbox)\n",
    "        img_face_ds = cv2.resize(img_face, (input_size[0], input_size[1]))\n",
    "        img_face_ds = cv2.cvtColor(img_face_ds, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        predictions = model.predict(\n",
    "            np.reshape(img_face_ds, [1, img_face_ds.shape[0], img_face_ds.shape[1], 3]))\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        pred_class = class_names[np.argmax(score)]\n",
    "        pred_score = 100 * np.max(score)\n",
    "        cv2.rectangle(\n",
    "            frame, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (255, 0, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame, pred_class + ' | ' + str(int(pred_score)) + '%', org=(bbox[0]+bbox[2], bbox[1]), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=1, color=(255, 0, 0))\n",
    "\n",
    "    cv2.imshow('Input', frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
