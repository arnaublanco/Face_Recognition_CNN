{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220ab5c5",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da64c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3036cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model already trained, then 0; otherwise, 1.\n",
    "train_model = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81ef30",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69642b",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b046cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "574 527 380 [574 527 380]\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    # Paths to images\n",
    "    image_path_arnau = \"data/Arnau\"\n",
    "    image_path_ashley = \"data/Ashley\"\n",
    "    image_path_megha = \"data/Megha\"\n",
    "\n",
    "    # Prepare list with image paths\n",
    "    file_list_arnau = os.listdir(image_path_arnau)\n",
    "    file_list_arnau = [ f for f in file_list_arnau if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "\n",
    "    file_list_ashley = os.listdir(image_path_ashley)\n",
    "    file_list_ashley = [ f for f in file_list_ashley if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "\n",
    "    file_list_megha = os.listdir(image_path_megha)\n",
    "    file_list_megha = [ f for f in file_list_megha if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "\n",
    "    # Join lists and create labels\n",
    "    file_list = file_list_arnau + file_list_ashley + file_list_megha\n",
    "    labels = np.array([0] * len(file_list_arnau) + [1] * len(file_list_ashley) + [2] * len(file_list_megha))\n",
    "\n",
    "    # Check that file_list and labels have the same shape\n",
    "    print(len(file_list) == len(labels))\n",
    "\n",
    "    # Print length for every group of faces\n",
    "    unique_counts = np.unique(labels,return_counts=True)\n",
    "    print(len(file_list_arnau),len(file_list_ashley),len(file_list_megha),unique_counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b6e8c",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa2835",
   "metadata": {},
   "source": [
    "### Functions for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5b3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find center of boundary box and return it.\n",
    "def findCenterBbox(bbox):\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_max = x_min + w\n",
    "    y_max = y_min + h\n",
    "\n",
    "    center = [int((x_max + x_min)//2), int((y_max + y_min)//2)] \n",
    "\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdbaccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find face using the OpenCV library and return boundary box.\n",
    "def findFaces(img):\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faceDetection = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    results = faceDetection.detectMultiScale(imgGray, 1.1, 10, minSize=[int(0.25*imgGray.shape[0]),int(0.25*imgGray.shape[1])])\n",
    "\n",
    "    bboxs = []\n",
    "    \n",
    "    if len(results):\n",
    "        counter = 1\n",
    "        for bbox in results:\n",
    "            center_bbox = findCenterBbox(bbox)\n",
    "            bboxs.append([counter, bbox, center_bbox])\n",
    "            counter = counter + 1\n",
    "\n",
    "    return bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ddc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dimensions of a square box based on the center and the dimensions.\n",
    "def makeSquareCrop(x_min, y_min, w, h):\n",
    "    centerX = x_min + round(w/2)\n",
    "    centerY = y_min + round(h/2)\n",
    "    dim = max([w,h])\n",
    "\n",
    "    x_min = centerX - round(dim/2)\n",
    "    y_min = centerY - round(dim/2)\n",
    "\n",
    "    w = dim\n",
    "    h = dim\n",
    "\n",
    "    return x_min, y_min, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ca9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the boundaries are valid.\n",
    "def boundariesValidation(x, y):\n",
    "    x = max([x, 0])\n",
    "    y = max([y, 0])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01008579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop image and return it.\n",
    "def crop(img, bbox):\n",
    "    x_min, y_min, w, h = bbox\n",
    "\n",
    "    x_min, y_min, w, h = makeSquareCrop(x_min, y_min, w, h)        \n",
    "\n",
    "    x_min, y_min = boundariesValidation(x_min, y_min)\n",
    "\n",
    "    return img[y_min:y_min+h, x_min:x_min+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc73d2",
   "metadata": {},
   "source": [
    "### Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc6d27a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1480/1480 [01:10<00:00, 20.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] [444 172 218] 834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = [64,64] # Input size of image\n",
    "if train_model:\n",
    "\n",
    "    # Initialize img_feature to store the images\n",
    "    img_features = np.empty([1,input_size[0], input_size[1],3])\n",
    "    idx_keep = []\n",
    "\n",
    "    # For loop to iterate through all files\n",
    "    for i in tqdm(range(1,len(file_list))):\n",
    "\n",
    "        f = file_list[i] # Current file\n",
    "\n",
    "        # Path based on label\n",
    "        if labels[i] == 2:\n",
    "            path = image_path_megha + '/' + f\n",
    "        if labels[i] == 1:\n",
    "            path = image_path_ashley + '/' + f\n",
    "        elif labels[i] == 0:\n",
    "            path = image_path_arnau + '/' + f\n",
    "\n",
    "        # Skip iteration if image cannot be loaded\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        bboxs = findFaces(img) # Find face\n",
    "\n",
    "        # If no faces have been found, skip face\n",
    "        if(len(bboxs) == 0):\n",
    "            continue     \n",
    "\n",
    "        # Crop faces in square dimensions\n",
    "        img_face = crop(img, bboxs[0][1])\n",
    "\n",
    "        # Resize image into input_size and convert BGR to RGB\n",
    "        img_face_ds = cv2.resize(img_face, (input_size[0],input_size[1]))\n",
    "        img_face_ds = cv2.cvtColor(img_face_ds, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Reshape image and append it to img_features\n",
    "        features = np.reshape(img_face_ds,[1,input_size[0],input_size[1],3])\n",
    "        img_features = np.append(img_features,features,axis=0)\n",
    "\n",
    "        idx_keep.append(i) # Indices of images with detected faces\n",
    "\n",
    "    # Delete positions where no faces have been detected\n",
    "    img_features = np.delete(img_features,obj=0,axis=0)\n",
    "    labels = labels[idx_keep]\n",
    "\n",
    "    N_samples = img_features.shape[0] # Compute number of samples\n",
    "    \n",
    "    # Print number of detected faces for every class\n",
    "    unique_counts = np.unique(labels,return_counts=True)\n",
    "    print(unique_counts[0],unique_counts[1],len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276b031",
   "metadata": {},
   "source": [
    "# Deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8d67f",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN) architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8bbd1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 12:51:19.370966: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-03 12:51:19.371095: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "        tf.keras.layers.Rescaling(1./255, input_shape=((input_size[0], input_size[1], 3))),\n",
    "        tf.keras.layers.Conv2D(input_size[0], (3, 3), activation='relu', input_shape=(input_size[0], input_size[1], 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(np.unique(labels)))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265c03f",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7b781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 12:51:19.623520: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-03 12:51:19.860824: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - ETA: 0s - loss: 1.0054 - accuracy: 0.5462"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 12:51:22.227758: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 3s 75ms/step - loss: 1.0054 - accuracy: 0.5462 - val_loss: 0.9538 - val_accuracy: 0.5440\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 0.8504 - accuracy: 0.6147 - val_loss: 0.6848 - val_accuracy: 0.6400\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 0.5948 - accuracy: 0.7671 - val_loss: 0.5129 - val_accuracy: 0.8400\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 0.3651 - accuracy: 0.8545 - val_loss: 0.3904 - val_accuracy: 0.7800\n",
      "Epoch 5/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.2528 - accuracy: 0.9110 - val_loss: 0.2903 - val_accuracy: 0.8560\n",
      "Epoch 6/100\n",
      "19/19 [==============================] - 1s 32ms/step - loss: 0.2046 - accuracy: 0.9212 - val_loss: 0.2316 - val_accuracy: 0.9120\n",
      "Epoch 7/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 0.1417 - accuracy: 0.9572 - val_loss: 0.3218 - val_accuracy: 0.8280\n",
      "Epoch 8/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 0.1947 - accuracy: 0.9229 - val_loss: 0.1820 - val_accuracy: 0.9320\n",
      "Epoch 9/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 0.1060 - accuracy: 0.9709 - val_loss: 0.1106 - val_accuracy: 0.9680\n",
      "Epoch 10/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0733 - accuracy: 0.9760 - val_loss: 0.1363 - val_accuracy: 0.9360\n",
      "Epoch 11/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 0.0509 - accuracy: 0.9846 - val_loss: 0.0720 - val_accuracy: 0.9840\n",
      "Epoch 12/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 0.0428 - accuracy: 0.9897 - val_loss: 0.0988 - val_accuracy: 0.9600\n",
      "Epoch 13/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0276 - accuracy: 0.9914 - val_loss: 0.1596 - val_accuracy: 0.9440\n",
      "Epoch 14/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 0.0246 - accuracy: 0.9966 - val_loss: 0.1333 - val_accuracy: 0.9400\n",
      "Epoch 15/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 0.0261 - accuracy: 0.9897 - val_loss: 0.0743 - val_accuracy: 0.9760\n",
      "Epoch 16/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.0776 - val_accuracy: 0.9440\n",
      "Epoch 17/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9983 - val_loss: 0.0593 - val_accuracy: 0.9680\n",
      "Epoch 18/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0200 - accuracy: 0.9949 - val_loss: 0.0504 - val_accuracy: 0.9760\n",
      "Epoch 19/100\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0601 - val_accuracy: 0.9760\n",
      "Epoch 20/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0297 - accuracy: 0.9897 - val_loss: 0.0518 - val_accuracy: 0.9880\n",
      "Epoch 21/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 0.0522 - val_accuracy: 0.9840\n",
      "Epoch 22/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9640\n",
      "Epoch 23/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9760\n",
      "Epoch 24/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9760\n",
      "Epoch 25/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9760\n",
      "Epoch 26/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9760\n",
      "Epoch 27/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 0.9760\n",
      "Epoch 28/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 9.7497e-04 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9760\n",
      "Epoch 29/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 8.7707e-04 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9760\n",
      "Epoch 30/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 8.0522e-04 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 0.9760\n",
      "Epoch 31/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 7.3304e-04 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9760\n",
      "Epoch 32/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 6.7370e-04 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 0.9760\n",
      "Epoch 33/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 6.3502e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9760\n",
      "Epoch 34/100\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 5.7798e-04 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9760\n",
      "Epoch 35/100\n",
      "19/19 [==============================] - 1s 32ms/step - loss: 5.3783e-04 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9760\n",
      "Epoch 36/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.0472e-04 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9760\n",
      "Epoch 37/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 4.6930e-04 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9760\n",
      "Epoch 38/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 4.4666e-04 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9760\n",
      "Epoch 39/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 4.1466e-04 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9760\n",
      "Epoch 40/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 4.0547e-04 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9760\n",
      "Epoch 41/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 3.7815e-04 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9760\n",
      "Epoch 42/100\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 3.5049e-04 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9760\n",
      "Epoch 43/100\n",
      "19/19 [==============================] - 1s 32ms/step - loss: 3.3800e-04 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9760\n",
      "Epoch 44/100\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 3.1287e-04 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.9760\n",
      "Epoch 45/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 3.0020e-04 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 0.9760\n",
      "Epoch 46/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 2.9108e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9760\n",
      "Epoch 47/100\n",
      "19/19 [==============================] - 1s 32ms/step - loss: 2.7399e-04 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9800\n",
      "Epoch 48/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 2.7105e-04 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 0.9760\n",
      "Epoch 49/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 2.6424e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9800\n",
      "Epoch 50/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 2.3398e-04 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9800\n",
      "Epoch 51/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 2.2941e-04 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9800\n",
      "Epoch 52/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 2.1603e-04 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9800\n",
      "Epoch 53/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 2.0498e-04 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9800\n",
      "Epoch 54/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.9922e-04 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9760\n",
      "Epoch 55/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.9001e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9800\n",
      "Epoch 56/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 1.8283e-04 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.9800\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 29ms/step - loss: 1.7447e-04 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9800\n",
      "Epoch 58/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.6818e-04 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.9800\n",
      "Epoch 59/100\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 1.6131e-04 - accuracy: 1.0000 - val_loss: 0.0553 - val_accuracy: 0.9800\n",
      "Epoch 60/100\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 1.5414e-04 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 0.9800\n",
      "Epoch 61/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.4825e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9800\n",
      "Epoch 62/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.4384e-04 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9800\n",
      "Epoch 63/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.3836e-04 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9800\n",
      "Epoch 64/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.3379e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9800\n",
      "Epoch 65/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.2885e-04 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9800\n",
      "Epoch 66/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 1.2526e-04 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9800\n",
      "Epoch 67/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.2020e-04 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9800\n",
      "Epoch 68/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.1597e-04 - accuracy: 1.0000 - val_loss: 0.0575 - val_accuracy: 0.9800\n",
      "Epoch 69/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.1306e-04 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9800\n",
      "Epoch 70/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 1.0926e-04 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9800\n",
      "Epoch 71/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.0617e-04 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9800\n",
      "Epoch 72/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 1.0252e-04 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9800\n",
      "Epoch 73/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 9.8822e-05 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9800\n",
      "Epoch 74/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 9.6783e-05 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 0.9800\n",
      "Epoch 75/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 9.3157e-05 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9800\n",
      "Epoch 76/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 9.0370e-05 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 0.9800\n",
      "Epoch 77/100\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 8.7722e-05 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 0.9800\n",
      "Epoch 78/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 8.6486e-05 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 0.9800\n",
      "Epoch 79/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 8.3585e-05 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9800\n",
      "Epoch 80/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 8.0401e-05 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9800\n",
      "Epoch 81/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 8.0536e-05 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9800\n",
      "Epoch 82/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 7.7719e-05 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9800\n",
      "Epoch 83/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 7.4623e-05 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9800\n",
      "Epoch 84/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 7.3850e-05 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9800\n",
      "Epoch 85/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 7.0254e-05 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9800\n",
      "Epoch 86/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 6.9643e-05 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9800\n",
      "Epoch 87/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 6.7250e-05 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 0.9800\n",
      "Epoch 88/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 6.5698e-05 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 0.9800\n",
      "Epoch 89/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 6.3892e-05 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9800\n",
      "Epoch 90/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 6.2371e-05 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9800\n",
      "Epoch 91/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 6.0560e-05 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9800\n",
      "Epoch 92/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.9593e-05 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9800\n",
      "Epoch 93/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.8534e-05 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9800\n",
      "Epoch 94/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.6744e-05 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 0.9800\n",
      "Epoch 95/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.5658e-05 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9800\n",
      "Epoch 96/100\n",
      "19/19 [==============================] - 1s 30ms/step - loss: 5.4287e-05 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9800\n",
      "Epoch 97/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.3125e-05 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9800\n",
      "Epoch 98/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.1471e-05 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9800\n",
      "Epoch 99/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.0359e-05 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9800\n",
      "Epoch 100/100\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 5.0624e-05 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9800\n",
      "8/8 - 0s - loss: 0.0605 - accuracy: 0.9800 - 73ms/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_model:\n",
    "    # Define indices of training (70%) and testing (30%)\n",
    "    train_indices = random.sample(range(0, N_samples), round(N_samples*0.7)) #int(np.floor(img_features.shape[0]*0.7))\n",
    "    test_indices = list(set(list(range(0,N_samples))).difference(set(train_indices)))\n",
    "\n",
    "    # Split images into training and testing\n",
    "    train = img_features[train_indices,:,:,:]\n",
    "    train_labels = labels[train_indices]\n",
    "\n",
    "    test = img_features[test_indices,:,:,:]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    # Train CNN model\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train, train_labels, epochs=100, validation_data=(test, test_labels))\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a11eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 12:52:17.962273: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    # Save model\n",
    "    model.save('data/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb542da",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cda6ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:19:40.197653: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('data/my_model') # Import model\n",
    "cap = cv2.VideoCapture(0) # Load webcam\n",
    "class_names = ['Arnau', 'Ashley', 'Megha'] # Classes names\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Real-time face recognition\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=0.5, fy=0.5,\n",
    "                       interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    bboxs = findFaces(frame)\n",
    "\n",
    "    c = cv2.waitKey(1)\n",
    "\n",
    "    if c == 27:  # Escape\n",
    "        cap.release()\n",
    "        break\n",
    "\n",
    "    for i in range(0, len(bboxs)):\n",
    "        bbox = bboxs[i][1]\n",
    "        img_face = crop(frame, bbox)\n",
    "        img_face_ds = cv2.resize(img_face, (input_size[0], input_size[1]))\n",
    "        img_face_ds = cv2.cvtColor(img_face_ds, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        predictions = model.predict(\n",
    "            np.reshape(img_face_ds, [1, img_face_ds.shape[0], img_face_ds.shape[1], 3]))\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        pred_class = class_names[np.argmax(score)]\n",
    "        pred_score = 100 * np.max(score)\n",
    "        cv2.rectangle(\n",
    "            frame, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (255, 0, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame, pred_class + ' | ' + str(int(pred_score)) + '%', org=(bbox[0]+bbox[2], bbox[1]), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=1, color=(255, 0, 0))\n",
    "\n",
    "    cv2.imshow('Input', frame)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "_ = cv2.waitKey(1) # Bug for MacOS (window does not close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de746e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
