{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model already trained, then 0; otherwise, 1.\n",
    "train_model = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    # Paths to images\n",
    "    image_path_arnau = \"data/Arnau\"\n",
    "    image_path_ashley = \"data/Ashley\"\n",
    "    image_path_megha = \"data/Megha\"\n",
    "\n",
    "    # Prepare list with image paths\n",
    "    file_list_arnau = os.listdir(image_path_arnau)\n",
    "    file_list_arnau = [ f for f in file_list_arnau if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "\n",
    "    file_list_ashley = os.listdir(image_path_ashley)\n",
    "    file_list_ashley = [ f for f in file_list_ashley if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "\n",
    "    file_list_megha = os.listdir(image_path_megha)\n",
    "    file_list_megha = [ f for f in file_list_megha if f.lower().endswith('.jpeg') or f.lower().endswith('.jpg')]\n",
    "\n",
    "    # Join lists and create labels\n",
    "    file_list = file_list_arnau + file_list_ashley + file_list_megha\n",
    "    labels = np.array([0] * len(file_list_arnau) + [1] * len(file_list_ashley) + [2] * len(file_list_megha))\n",
    "\n",
    "    # Check that file_list and labels have the same shape\n",
    "    print(len(file_list) == len(labels))\n",
    "\n",
    "    # Print length for every group of faces\n",
    "    unique_counts = np.unique(labels,return_counts=True)\n",
    "    print(len(file_list_arnau),len(file_list_ashley),len(file_list_megha),unique_counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find center of boundary box and return it.\n",
    "def findCenterBbox(bbox):\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_max = x_min + w\n",
    "    y_max = y_min + h\n",
    "\n",
    "    center = [int((x_max + x_min)//2), int((y_max + y_min)//2)] \n",
    "\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find face using the OpenCV library and return boundary box.\n",
    "def findFaces(img):\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faceDetection = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    results = faceDetection.detectMultiScale(imgGray, 1.1, 10, minSize=[int(0.25*imgGray.shape[0]),int(0.25*imgGray.shape[1])])\n",
    "\n",
    "    bboxs = []\n",
    "    \n",
    "    if len(results):\n",
    "        counter = 1\n",
    "        for bbox in results:\n",
    "            center_bbox = findCenterBbox(bbox)\n",
    "            bboxs.append([counter, bbox, center_bbox])\n",
    "            counter = counter + 1\n",
    "\n",
    "    return bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dimensions of a square box based on the center and the dimensions.\n",
    "def makeSquareCrop(x_min, y_min, w, h):\n",
    "    centerX = x_min + round(w/2)\n",
    "    centerY = y_min + round(h/2)\n",
    "    dim = max([w,h])\n",
    "\n",
    "    x_min = centerX - round(dim/2)\n",
    "    y_min = centerY - round(dim/2)\n",
    "\n",
    "    w = dim\n",
    "    h = dim\n",
    "\n",
    "    return x_min, y_min, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the boundaries are valid.\n",
    "def boundariesValidation(x, y):\n",
    "    x = max([x, 0])\n",
    "    y = max([y, 0])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop image and return it.\n",
    "def crop(img, bbox):\n",
    "    x_min, y_min, w, h = bbox\n",
    "\n",
    "    x_min, y_min, w, h = makeSquareCrop(x_min, y_min, w, h)        \n",
    "\n",
    "    x_min, y_min = boundariesValidation(x_min, y_min)\n",
    "\n",
    "    return img[y_min:y_min+h, x_min:x_min+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_size = [64,64] # Input size of image\n",
    "if train_model:\n",
    "\n",
    "    # Initialize img_feature to store the images\n",
    "    img_features = np.empty([1,input_size[0], input_size[1],3])\n",
    "    idx_keep = []\n",
    "\n",
    "    # For loop to iterate through all files\n",
    "    for i in tqdm(range(1,len(file_list))):\n",
    "\n",
    "        f = file_list[i] # Current file\n",
    "\n",
    "        # Path based on label\n",
    "        if labels[i] == 2:\n",
    "            path = image_path_megha + '/' + f\n",
    "        if labels[i] == 1:\n",
    "            path = image_path_ashley + '/' + f\n",
    "        elif labels[i] == 0:\n",
    "            path = image_path_arnau + '/' + f\n",
    "\n",
    "        # Skip iteration if image cannot be loaded\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        bboxs = findFaces(img) # Find face\n",
    "\n",
    "        # If no faces have been found, skip face\n",
    "        if(len(bboxs) == 0):\n",
    "            continue     \n",
    "\n",
    "        # Crop faces in square dimensions\n",
    "        img_face = crop(img, bboxs[0][1])\n",
    "\n",
    "        # Resize image into input_size and convert BGR to RGB\n",
    "        img_face_ds = cv2.resize(img_face, (input_size[0],input_size[1]))\n",
    "        img_face_ds = cv2.cvtColor(img_face_ds, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Reshape image and append it to img_features\n",
    "        features = np.reshape(img_face_ds,[1,input_size[0],input_size[1],3])\n",
    "        img_features = np.append(img_features,features,axis=0)\n",
    "\n",
    "        idx_keep.append(i) # Indices of images with detected faces\n",
    "\n",
    "    # Delete positions where no faces have been detected\n",
    "    img_features = np.delete(img_features,obj=0,axis=0)\n",
    "    labels = labels[idx_keep]\n",
    "\n",
    "    N_samples = img_features.shape[0] # Compute number of samples\n",
    "    \n",
    "    # Print number of detected faces for every class\n",
    "    unique_counts = np.unique(labels,return_counts=True)\n",
    "    print(unique_counts[0],unique_counts[1],len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN) architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "        tf.keras.layers.Rescaling(1./255, input_shape=((input_size[0], input_size[1], 3))),\n",
    "        tf.keras.layers.Conv2D(input_size[0], (3, 3), activation='relu', input_shape=(input_size[0], input_size[1], 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(np.unique(labels)))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    # Define indices of training (70%) and testing (30%)\n",
    "    train_indices = random.sample(range(0, N_samples), round(N_samples*0.7)) #int(np.floor(img_features.shape[0]*0.7))\n",
    "    test_indices = list(set(list(range(0,N_samples))).difference(set(train_indices)))\n",
    "\n",
    "    # Split images into training and testing\n",
    "    train = img_features[train_indices,:,:,:]\n",
    "    train_labels = labels[train_indices]\n",
    "\n",
    "    test = img_features[test_indices,:,:,:]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    # Train CNN model\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train, train_labels, epochs=100, validation_data=(test, test_labels))\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    # Save model\n",
    "    model.save('data/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 15:08:44.692776: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-11 15:08:44.698644: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-03-11 15:08:47.539042: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-11 15:08:47.615825: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('data/my_model') # Import model\n",
    "cap = cv2.VideoCapture(0) # Load webcam\n",
    "class_names = ['Arnau', 'Ashley', 'Megha'] # Classes names\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Real-time face recognition\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=0.5, fy=0.5,\n",
    "                       interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    bboxs = findFaces(frame)\n",
    "\n",
    "    c = cv2.waitKey(1)\n",
    "\n",
    "    if c == 27:  # Escape\n",
    "        cap.release()\n",
    "        break\n",
    "\n",
    "    for i in range(0, len(bboxs)):\n",
    "        bbox = bboxs[i][1]\n",
    "        img_face = crop(frame, bbox)\n",
    "        img_face_ds = cv2.resize(img_face, (input_size[0], input_size[1]))\n",
    "        img_face_ds = cv2.cvtColor(img_face_ds, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        predictions = model.predict(\n",
    "            np.reshape(img_face_ds, [1, img_face_ds.shape[0], img_face_ds.shape[1], 3]))\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        pred_class = class_names[np.argmax(score)]\n",
    "        pred_score = 100 * np.max(score)\n",
    "        cv2.rectangle(\n",
    "            frame, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (255, 0, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame, pred_class + ' | ' + str(int(pred_score)) + '%', org=(bbox[0]+bbox[2], bbox[1]), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=1, color=(255, 0, 0))\n",
    "\n",
    "    cv2.imshow('Input', frame)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "_ = cv2.waitKey(1) # Bug for MacOS (window does not close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
